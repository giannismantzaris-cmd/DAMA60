{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6m27uwoNW2RT5V28eV37H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giannismantzaris-cmd/DAMA60/blob/main/Topic5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oYbDYCEPTCc",
        "outputId": "52dc3594-f8dc-413c-8900-8723a4eab2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DAMA60' already exists and is not an empty directory.\n",
            "=== Topic 5: Flajolet–Martin on a Wikipedia page ===\n",
            "Enter Wikipedia URL (or press Enter for default 'Data science wikipedia page'): Traceback (most recent call last):\n",
            "object address  : 0x7842e2431d20\n",
            "object refcount : 3\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/giannismantzaris-cmd/DAMA60.git\n",
        "!python DAMA60/WA2/Topic5.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DAMA60 HW2 Topic5\n",
        "#Topic 5 – Python\n",
        "#Flajolet–Martin distinct word estimation from a Wikipedia page.\n",
        "\n",
        "#How to run (IDLE or terminal):\n",
        "#- Just run this file. You will be prompted for a Wikipedia URL.\n",
        "#  Example: https://en.wikipedia.org/wiki/Data_science\n",
        "#- If you press Enter, a default URL will be used.\n",
        "\n",
        "#No third-party libraries required.\n",
        "\n",
        "#Import needed libraries\n",
        "import re\n",
        "import sys\n",
        "import html\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "import statistics\n",
        "from typing import List, Tuple\n",
        "\n",
        "# ---------------------------- Text fetching & cleaning ----------------------------\n",
        "\n",
        "def fetch_text(url: str):\n",
        "    #Fetch raw HTML from URL and return a best-effort cleaned plain text.\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
        "                                 \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                                 \"Chrome/120.0 Safari/537.36\"}\n",
        "        req = urllib.request.Request(url, headers=headers)\n",
        "        with urllib.request.urlopen(req, timeout=20) as resp:\n",
        "            charset = resp.headers.get_content_charset() or \"utf-8\"\n",
        "            html_bytes = resp.read()\n",
        "    except urllib.error.URLError as e:\n",
        "        print(f\"[Error] Failed to fetch URL: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "    page = html_bytes.decode(charset, errors=\"replace\")# Decode the raw HTML bytes into a string using the detected character set (e.g., UTF-8).\n",
        "                                                        # If some characters cannot be decoded, they are replaced with a safe placeholder.\n",
        "    page = html.unescape(page) # Convert HTML entities (like &amp;, &lt;, &gt;) into their actual characters (&, <, >).\n",
        "    page = re.sub(r\"(?is)<script.*?>.*?</script>\", \" \", page) # Remove <script>...</script> blocks (JavaScript code).\n",
        "                                                                # (?is) makes the regex case-insensitive and allows to match newlines.\n",
        "    page = re.sub(r\"(?is)<style.*?>.*?</style>\", \" \", page) # Remove <style>...</style> blocks (CSS definitions).\n",
        "    page = re.sub(r\"(?is)<!--.*?-->\", \" \", page) # Remove HTML comments <!-- ... -->.\n",
        "    text = re.sub(r\"(?s)<[^>]+>\", \" \", page) # Remove any remaining HTML tags like <div>, <p>, <a>, etc.\n",
        "                                                # (?s) makes . match newlines too.\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # Collapse multiple spaces, tabs, or newlines into a single space.\n",
        "                                                # Also strip leading and trailing whitespace.\n",
        "    return text\n",
        "\n",
        "#Tokenization\n",
        "def tokenize_words(text: str): # The function will return a list of strings (tokens).\n",
        "    text = text.lower() # Lowercase the text so that \"Data\" and \"data\" are treated as the same word\n",
        "    for ch in text:\n",
        "        if not (ch.isalpha() or ch in \"'-\"): #Checks if the character is a letter, apostrophe, or hyphen\n",
        "            text = text.replace(ch, \" \")# Replace any character that is NOT a letter (a–z, A–Z), apostrophe ('), or hyphen (-)\n",
        "                                            # with a space. This removes digits, punctuation, symbols, etc.\n",
        "    tokens = [re.sub(r\"(^[-']+|[-']+$)\", \"\", t) for t in text.split()] # Split into tokens (words) and remove leading/trailing apostrophes or hyphens\n",
        "                                                                        # Example: \"'hello-'\" → \"hello\"\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "       if t:                # keep only non-empty\n",
        "        cleaned.append(t)\n",
        "    return cleaned # Return the cleaned list of tokens, excluding any empty strings\n",
        "\n",
        "#Custom hash function\n",
        "def myHash(text: str):\n",
        "    hash = 0\n",
        "    for ch in text:\n",
        "        hash = (hash * 281 ^ ord(ch) * 997) & 0xFFFFFFFF #281 and 997 are prime numbers\n",
        "    return hash\n",
        "\n",
        "# ---------------------------- Flajolet–Martin ----------------------------\n",
        "\n",
        "def trailing_zeros(n: int):\n",
        "    #Count trailing zero bits in n (for n > 0). If n == 0, return 64.\n",
        "    if n == 0:\n",
        "        return 64\n",
        "    tz = 0\n",
        "    while n & 1 == 0: # while least significant bit is 0\n",
        "        tz = tz + 1\n",
        "        n = n >> 1 # shift right (divide by 2), move to next bit\n",
        "    return tz\n",
        "\n",
        "def fm_estimate(words: List[str], hash_params: List[Tuple[int, int]], m_bits: int = 32): # hash_params: List[Tuple[int, int]]: A list of tuples, each containing two integers (a, b).\n",
        "                                                                                        # Each (a, b) pair defines one hash function of the form h(x) = (a * x + b) mod M.\n",
        "                                                                                        # m_bits: Sets the number of bits in the hash range (M = 2^32).\n",
        "    #Run Flajolet–Martin with multiple (a,b) linear hashes.\n",
        "    # This function takes a stream of words, applies multiple hash functions, and estimates the number of distinct elements using the Flajolet–Martin algorithm\n",
        "    M = 2** m_bits #Compute M = 2 ** m_bits\n",
        "    max_tz_per_hash = [0] * len(hash_params) #Initialize a list of zeros, one entry per hash function, to store the maximum trailing zeros\n",
        "\n",
        "    for w in words: #look at each word from the text one by one\n",
        "\n",
        "        word_hash = myHash(w) # Convert the word into an integer using the custom hash function myHash defined above\n",
        "        for i, (a, b) in enumerate(hash_params):\n",
        "            hv = (a*word_hash+b) % M   # Compute hash value in range [0, M)\n",
        "            r = trailing_zeros(hv)# Count trailing zeros in binary form of hv\n",
        "\n",
        "            if r > max_tz_per_hash[i]: # Update max trailing zeros for this hash function\n",
        "                max_tz_per_hash[i] = r\n",
        "\n",
        "    estimates = [2 ** r for r in max_tz_per_hash] # Convert max trailing zeros into distinct-count estimates\n",
        "    combined = statistics.median(estimates)  # Compute median using python library statistics\n",
        "\n",
        "    return max_tz_per_hash, estimates, combined\n",
        "\n",
        "# ---------------------------- Main program ----------------------------\n",
        "\n",
        "print(\"=== Topic 5: Flajolet–Martin on a Wikipedia page ===\")\n",
        "url = input(\"Enter Wikipedia URL (or press Enter for default 'Data science wikipedia page'): \").strip()\n",
        "if not url:\n",
        "    url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
        "\n",
        "print(f\"[1/4] Fetching page: {url}\")\n",
        "text = fetch_text(url)\n",
        "if not text:\n",
        "    print(\"No text fetched. Exiting.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"[2/4] Tokenizing words...\")\n",
        "words = tokenize_words(text) #cleans the page text and returns a list of words (tokens).\n",
        "total_tokens = len(words) #total number of tokens in the text (including duplicates).\n",
        "unique_tokens = len(set(words)) # set(words): removes duplicates, leaving only distinct words.\n",
        "                                #len(set(words)): the exact number of unique words.\n",
        "print(f\"Tokens: {total_tokens:,} | Unique (exact set): {unique_tokens:,}\")\n",
        "\n",
        "# Independent hash functions (different (a,b) pairs), using hexadecimal constants\n",
        "# Each pair represents the coefficients of a simple linear hash function\n",
        "print(\"[3/4] Running Flajolet–Martin...\")\n",
        "\n",
        "# The constants a and b are written in hexadecimal for compactness.\n",
        "#They are just big integers used to define different hash functions.\n",
        "#Hex is common in hashing\n",
        "hash_params = [             # The values are written in hexadecimal (like 0x27d4eb2f), but they’re just big integers.\n",
        "    (0x27d4eb2f, 0x165667b1),\n",
        "    (0x9e3779b1, 0x85ebca6b),\n",
        "    (0xc2b2ae35, 0x27d4eb2f),\n",
        "    (0x165667b1, 0x9e3779b1),\n",
        "    (0x85ebca6b, 0xc2b2ae35),\n",
        "    (0x27d4eb2f, 0x85ebca6b),\n",
        "    (0x9e3779b1, 0x27d4eb2f),\n",
        "    (0xc2b2ae35, 0x165667b1),\n",
        "    (0x2545f491, 0x9e3779b1),\n",
        "    (0x94d049bb, 0xc2b2ae35),\n",
        "    (0x369dea0f, 0x94d049bb),\n",
        "    (0x7f4a7c15, 0x2545f491),\n",
        "]\n",
        "max_tz, ests, combined = fm_estimate(words, hash_params, m_bits=32)\n",
        "\n",
        "print(\"[4/4] Results\")\n",
        "print(\"Hash # | max trailing zeros (R) | estimate (2^R)\")\n",
        "i = 1\n",
        "for index in range(len(max_tz)):\n",
        "    r = max_tz[index]\n",
        "    e = ests[index]\n",
        "    print(f\"Hash {i:<2} | R = {r:<3} | Estimate = {e:<8}\")\n",
        "    i += 1\n",
        "print(f\"\\nCombined Flajolet–Martin estimate (median): {combined:,.2f} distinct words\")\n",
        "print(f\"Exact unique tokens(set)             : {unique_tokens:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzX5YEXDQiZs",
        "outputId": "e8998bcb-9804-4c3b-d314-f7152559206b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Topic 5: Flajolet–Martin on a Wikipedia page ===\n"
          ]
        }
      ]
    }
  ]
}